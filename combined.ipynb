{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "\"\"\"\n", "Combined script\n", "Created on Sun Dec 12 18:08:10 2021"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["@author: Raska\n", "\"\"\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[3]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import findspark\n", "findspark.init()\n", "import pyspark\n", "from pyspark.sql.functions import col, row_number, lit,udf\n", "findspark.find()\n", "import dateutil.parser\n", "# from pyspark import SparkContext, SparkConf\n", "from pyspark.sql import SparkSession\n", "conf = pyspark.SparkConf().setAppName('SparkApp').setMaster('local')\n", "sc = pyspark.SparkContext(conf=conf)\n", "spark = SparkSession(sc)\n", "# from functools import reduce\n", "from pyspark.sql import functions as F "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pyspark.sql.window import Window\n", "from pyspark.sql import types as T"]}, {"cell_type": "markdown", "metadata": {}, "source": ["% ##### TASK 1 ######"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#This function takes in 1 parameter data(the dataset used for this project), and returns a pyspark dataframe containing the average daily cases for each month per country."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def mean_over_month(data):\n", "    #dropping unwanted columns and grouping by countries\n", "    df = data.drop(\"Province/State\",\"Lat\",\"Long\").groupBy(\"Country/Region\").sum()\n", "    #separating the country column and dates column\n", "    country_region = df.select(df.columns[0])\n", "    data_by_dates = df.select(df.columns[1:])\n", "    #Renaming date columns after groupby\n", "    headers_before_groupby =  data.select(data.columns[4:]).columns\n", "    headers_after_groupby = data_by_dates.columns\n", "    mapping = dict(zip(headers_after_groupby,headers_before_groupby))\n", "    renamed_frame_afterGroupby = data_by_dates.select([F.col(c).alias(mapping.get(c, c)) for c in data_by_dates.columns])\n", "    #Getting the last date in a month as the other days are redundant for finding the mean\n", "    lastdates = []\n", "    for i in range(len(headers_before_groupby)):\n", "        try:\n", "            if headers_before_groupby[i+1].split(\"/\")[0] != headers_before_groupby[i].split(\"/\")[0]:\n", "                lastdates.append(headers_before_groupby[i])\n", "        except:\n", "            lastdates.append(headers_before_groupby[-1])\n", "    #concatenating the number of days in each month    \n", "    lstdat = []\n", "    for i in lastdates:\n", "        if i == '1/31/20':\n", "            lstdat.append(i+\"-8\"+\"-days\")\n", "        else:\n", "            lstdat.append(i+\"-\"+i.split(\"/\")[1]+\"-days\")\n", "        \n", "    Data_datesLast = renamed_frame_afterGroupby.select(lastdates)\n", "    #renaming multiple columns\n", "    from pyspark.sql.functions import col\n", "    mapping = dict(zip(lastdates,lstdat))\n", "    Last_dates_data = Data_datesLast.select([col(c).alias(mapping.get(c, c)) for c in Data_datesLast.columns])\n", "    #initializing variables to assist in joining dataframes\n", "    w = Window.partitionBy(lit(1)).orderBy(lit(1))\n", "    ls = Last_dates_data.columns\n", "    fin = Last_dates_data.select(Last_dates_data.columns[0])\n", "    DF1 = fin.withColumn(\"row_id\", row_number().over(w))\n", "    #looping though the Dataframe and calculating the mean, and then joining\n", "    for i in range(len(ls)):\n", "        if i == 0:\n", "            pass\n", "        else:\n", "            mean_fullframe = Last_dates_data.withColumn(ls[i],(F.col(ls[i])-F.col(ls[i-1]))/int(ls[i].split(\"-\")[1]))\n", "            mean_singleframe = mean_fullframe.select(mean_fullframe.columns[i])\n", "            DF3 = mean_singleframe.withColumn(\"row_id\", row_number().over(w))\n", "            DF1 = DF1.join(DF3, (\"row_id\"))\n", "    #adding an index to country/region to join\n", "    country = country_region.withColumn(\"row_id\", row_number().over(w))\n", "    #joining the country with the remaining dates dataframe\n", "    Final = DF1.join(country, on=\"row_id\", how='full').drop(\"row_id\")\n", "    #Rearranging the columns and getting the final result\n", "    lst = Final.columns\n", "    Mean_perCountry_perMonth = Final.select(lst[-1:] + lst[:-1])\n", "    return Mean_perCountry_perMonth"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%<br>\n", "#This function plots a horizontal bar graph and takes in 2 parameters, data(the pandas dataframe to be plotted) and the title(the title of the plot)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_func_bar(data,title):\n", "    ax = data.plot.barh(stacked=True,figsize=(10,10),title=title)\n", "    ax.set_xlabel('AVERAGE CASES PER DAY')\n", "    ax.set_ylabel('MONTHS')\n", "###This function plots a line graph and takes in 2 parameters, data(the pandas dataframe to be plotted) and the title(the title of the plot)    \n", "def plot_func_line(data,title):\n", "    ax = data.plot.line(figsize=(10,10),title=title)\n", "    ax.set_xlabel('MONTHS')\n", "    ax.set_ylabel('AVERAGE CASES PER DAY')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%<br>\n", "Reading the csv from the local"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = spark.read.csv('data.csv', header=True, inferSchema=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ncomment if you need to add the cases of summer olympics 2020 to Japan<br>\n", "ata = data.withColumn(\"Country/Region\",when(col(\"Country/Region\") == \"Summer Olympics 2020\",\"Japan\").otherwise(col(\"Country/Region\")))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%<br>\n", "alling the function to get the mean values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mean_values = mean_over_month(data)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%<br>\n", "resenting the data in a better format"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pretty_mean = mean_values.toPandas()\n", "pretty_mean.rename(columns={'1/31/20-8-days': '1/20', '2/29/20-29-days': '2/20', '3/31/20-31-days': '3/20', '4/30/20-30-days': '4/20', '5/31/20-31-days': '5/20', '6/30/20-30-days': '6/20','7/31/20-31-days':'7/20','8/31/20-31-days':'8/20','9/30/20-30-days':'9/20','10/31/20-31-days':'10/20','11/30/20-30-days':'11/20','12/31/20-31-days':'12/20','1/31/21-31-days':'1/21','2/28/21-28-days':'2/21','3/31/21-31-days':'3/21','4/30/21-30-days':'4/21','5/31/21-31-days':'5/21','6/30/21-30-days':'6/21','7/31/21-31-days':'7/21','8/31/21-31-days':'8/21','9/30/21-30-days':'9/21','10/31/21-31-days':'10/21','11/23/21-23-days':'11/21'}, inplace=True)\n", "final = pretty_mean.set_index('Country/Region').T"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%<br>\n", "electing the countries with the highest and the lowest number of cases"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["most_cases = final[['US','India','Brazil','United Kingdom','Russia','Turkey','France','Germany','Iran','Argentina']]\n", "least_cases = final[['Micronesia','Tonga','Kiribati','Samoa','Marshall Islands','Vanuatu','Palau','MS Zaandam','Solomon Islands','Holy See']]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["final.to_csv('mean_values.csv')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%<br>\n", "plotting the graph for the countries with the highest number of cases by calling plot_func function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_func_bar(most_cases,\"DISTRIBUTION OF AVERAGE DAILY CASES PER MONTH FOR THE MOST AFFECTED COUNTRIES\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#plotting the graph for the countries with the highest number of cases by calling plot_func function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_func_bar(least_cases,\"DISTRIBUTION OF AVERAGE DAILY CASES PER MONTH FOR THE LEAST AFFECTED COUNTRIES\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%"]}, {"cell_type": "markdown", "metadata": {}, "source": ["plotting the graph for the countries with the highest number of cases by calling plot_func function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_func_line(most_cases,\"AVERAGE DAILY CASES PER MONTH FOR THE MOST AFFECTED COUNTRIES\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%<br>\n", "#plotting the graph for the countries with the least number of cases by calling plot_func function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_func_line(least_cases,\"AVERAGE DAILY CASES PER MONTH FOR THE LEAST AFFECTED COUNTRIES\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[3]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ropping unwanted columns and grouping by countries"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = data"]}, {"cell_type": "markdown", "metadata": {}, "source": [".groupBy(\"Province/State\").sum()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[4]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w = Window.partitionBy(lit(1)).orderBy(lit(1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["eparating the country column and dates column"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["country_region = df.select(df.columns[:4])\n", "data_by_dates = df.select(df.columns[4:])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[5]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["enaming date columns after groupby"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["headers_before_groupby =  data.select(data.columns[4:]).columns\n", "headers_after_groupby = data_by_dates.columns"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mapping = dict(zip(headers_after_groupby,headers_before_groupby))\n", "renamed_frame_afterGroupby = data_by_dates.select([col(c).alias(mapping.get(c, c)) for c in data_by_dates.columns])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[6]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["etting the last date in a month as the other days are redundant for finding the mean"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lastdates = []\n", "for i in range(len(headers_before_groupby)):\n", "    try:\n", "        if headers_before_groupby[i+1].split(\"/\")[0] != headers_before_groupby[i].split(\"/\")[0]:\n", "            lastdates.append(headers_before_groupby[i])\n", "    except:\n", "        lastdates.append(headers_before_groupby[-1])\n", "#concatenating the number of days in each month    \n", "lstdat = []\n", "for i in lastdates:\n", "    if i == '1/31/20':\n", "        lstdat.append(i+\"-8\"+\"-days\")\n", "    else:\n", "        lstdat.append(i+\"-\"+i.split(\"/\")[1]+\"-days\")\n", "        \n", "Data_datesLast = renamed_frame_afterGroupby.select(lastdates)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[7]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["enaming multiple columns"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pyspark.sql.functions import col"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mapping = dict(zip(lastdates,lstdat))\n", "Last_dates_data = Data_datesLast.select([col(c).alias(mapping.get(c, c)) for c in Data_datesLast.columns])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[8]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["nitializing variables to assist in joining dataframes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ls = Last_dates_data.columns\n", "fin = Last_dates_data.select(Last_dates_data.columns[0])\n", "DF1 = fin.withColumn(\"row_id\", row_number().over(w))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[9]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ooping though the Dataframe and calculating the mean, and then joining"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for i in range(len(ls)):\n", "    if i == 0:\n", "        pass\n", "    else:\n", "        mean_fullframe = Last_dates_data.withColumn(ls[i],(F.col(ls[i])-F.col(ls[i-1])))\n", "        mean_singleframe = mean_fullframe.select(mean_fullframe.columns[i])\n", "        DF3 = mean_singleframe.withColumn(\"row_id\", row_number().over(w))\n", "        \n", "        DF1 = DF1.join(DF3, (\"row_id\"))\n", "# -F.col(ls[i-1])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[10]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["dding an index to country/region to join"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["country = country_region.withColumn(\"row_id\", row_number().over(w))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[10]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["oining the country with the remaining dates dataframe"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Final = DF1.join(country, on=\"row_id\", how='full').drop(\"row_id\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[11]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["earranging the columns and getting the final result"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lst = Final.columns\n", "Final = Final.select(lst[-4:] + lst[:-4])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Thierry Q2 Daily increase"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[12]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["earranging the data from cumulative to daily increase"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["date_columns=list(data_by_dates.columns)\n", "for k in range(len(date_columns)-1):\n", "    data_by_dates=data_by_dates.withColumn(date_columns[k],(data_by_dates[k+1]-data_by_dates[k]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[13]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["rop the last column which had kept cumulative values."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_by_dates = data_by_dates.drop(\"11/18/21\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[14]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["efine standard trendline function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def linearTrendlineCoefficient(*args):\n", "    \"\"\"\n", "    Wrapper function of linear regression optimised for PySpark data\n", "    Parameters\n", "    ----------\n", "    *args : tuple\n", "        Unbounded list of data points.\n", "    Returns\n", "    -------\n", "    float\n", "        Linear coefficient of linear trendline fit of data points.\n", "    \"\"\"\n", "    from sklearn.linear_model import LinearRegression\n", "    import numpy as np\n", "    X = []\n", "    for value in args:\n", "        X.append(value)\n", "    X = np.array(X)\n", "    y = np.arange(len(X))\n", "    X = X.reshape((-1,1))\n", "    y = y.reshape((-1,1))\n", "    reg = LinearRegression().fit(y, X)\n", "    coef_array = reg.coef_\n", "    out = coef_array[0]\n", "    return float(out)\n", "#Convert to UDF\n", "getLinearTrendlineCoef = udf(lambda *args: linearTrendlineCoefficient(*args), T.FloatType())\n", "#Selecting columns for trendline\n", "df_coef = data_by_dates\n", "#Fitting trendline \n", "df_coef = df_coef.withColumn('Linear Coef', getLinearTrendlineCoef(*[F.col(i) for i in df_coef.columns]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[15]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Add a row id to both dataframes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_coef = df_coef.withColumn(\"row_id\", row_number().over(w))\n", "data_by_dates = data_by_dates.withColumn(\"row_id\", row_number().over(w))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[16]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Join both datarframes "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["country.join(df_coef.select('Linear Coef','row_id'), on='row_id', how='full_outer')\n", "df_all = country.join(df_coef, on=\"row_id\", how='full_outer').drop(\"row_id\")\n", "# Sort and filter the top 100 rows\n", "df_all = df_all.sort(F.col(\"Linear Coef\").desc())\n", "data_top100 = df_all.limit(100)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Continents<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[17]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "from matplotlib import path\n", "def getContinent(lon, lat):\n", "    '''\n", "    \n", "    Parameters\n", "    ----------\n", "    lon : Numbers\n", "        Longitude.\n", "    lat : Numbers\n", "        Latitude.\n", "    Returns\n", "    -------\n", "    String\n", "        Continent.\n", "    '''\n\n", "    # Coordinates - America\n", "    LonNAm = np.array([90,       90,  78.13,      57.5,  15,  15,  1.25,  1.25,  51,  60,    60])\n", "    LatNAm = np.array([-168.75, -10,    -10,     -37.5, -30, -75, -82.5, -105,  -180, -180, -168.75])\n", "    LatNA2 = np.array([51,    51,  60])\n", "    LonNA2 = np.array([166.6, 180, 180])\n", "    LatNA3 = np.array([22, 18, 19, 23])#hawaii\n", "    LonNA3 = np.array([-160, -160, -153, -153])#hawaii\n", "    LonSAm = np.array([1.25,  1.25,   15,  15, -60, -60])\n", "    LatSAm = np.array([-105, -82.5,  -75, -30, -30, -105])\n\n", "    # Coordinates - Europe\n", "    LonEur = np.array([90,   90,  42.5, 42.5, 40.79, 41, 40.55, 40.40, 40.05, 39.17, 35.687499, 35.46, 33,   38,  35.42, 28.25, 15,  57.5,  78.13])\n", "    LatEur = np.array([-10, 77.5, 48.8, 30,   28.81, 29, 27.31, 26.75, 26.36, 25.19, 13.911866, 27.91, 27.5, 10, -10,  -13,   -30, -37.5, -10])\n", "    LatEu1 = np.array([14.150906, 14.090299, 14.811997, 14.826364])\n", "    LonEu1 = np.array([36.304948, 35.741447, 35.710506, 36.195053])\n", "    \n", "    #Coordinates - Africa\n", "    LonAfr = np.array([15,  28.25, 35.42, 35.687499, 38, 33,   31.74, 29.54, 27.78, 11.3, 12.5, -60, -60])\n", "    LatAfr = np.array([-30, -13,   13.911866,-10, 10, 27.5, 34.58, 34.92, 34.46, 44.3, 52,    75, -30])\n", "    LonAf1 = np.array([32.035586, 32.035586, 31.338941, 31.338941])\n", "    LatAf1 = np.array ([-6.00000, - 8.338103, -8.338103, -6.00000])\n", "    \n", "    #Coordinates - Asia\n", "    LonAsi = np.array([90,   42.5, 42.5, 40.79, 41, 40.55, 40.4,  40.05, 39.17, 35.46, 33,   31.74, 29.54, 27.78, 11.3, 12.5, -60, -60, -31.88, -11.88, -10.27, 33.13, 51,    60,  90])\n", "    LatAsi = np.array([77.5, 48.8, 30,   28.81, 29, 27.31, 26.75, 26.36, 25.19, 27.91, 27.5, 34.58, 34.92, 34.46, 44.3, 52,   75,  110,  110,  110,    140,    140,   166.6, 180, 180])\n", "    LatAs2 = np.array([90,    90,      60,      60,])\n", "    LonAs2 = np.array([-180, -168.75, -168.75, -180,])\n", "    \n", "    #Coordinates - Antarctica\n", "    LonAnt = np.array([-60, -60, -90, -90])\n", "    LatAnt = np.array([-180, 180, 180, -180])\n", "    \n", "    def inContinent(xq, yq, xv, yv):\n", "        xq = np.array(xq)\n", "        yq = np.array(yq)\n", "        xv = np.array(xv)\n", "        yv = np.array(yv)\n", "        shape = xq.shape\n", "        xq = xq.reshape(-1)\n", "        yq = yq.reshape(-1)\n", "        xv = xv.reshape(-1)\n", "        yv = yv.reshape(-1)\n", "        q = [(xq[i], yq[i]) for i in range(xq.shape[0])]\n", "        p = path.Path([(xv[i], yv[i]) for i in range(xv.shape[0])])\n", "        return p.contains_points(q).reshape(shape)\n", "    def inNA(lat, lon):\n", "        if (lat==0 and lon==0) or (pd.isna(lat)==True or pd.isna(lon)==True):\n", "            return True\n", "        else:\n", "            return False\n", "    \n", "    #Checking truth values\n", "    inIntl = inNA(lat, lon)\n", "    \n", "    if inIntl==True:\n", "        return 'Not applicable'\n", "    else: \n", "        inNAm = inContinent(lon, lat, LonNAm, LatNAm)\n", "        inNA2 = inContinent(lon, lat, LonNA2, LatNA2)\n", "        inNA3 = inContinent(lon, lat, LonNA3, LatNA3)\n", "        inSAm = inContinent(lon, lat, LonSAm, LatSAm)\n", "    \n", "    if inNAm==True or inNA2==True or inNA3==True or inSAm==True:\n", "        return 'America'\n", "    else: \n", "        inEur = inContinent(lon, lat, LonEur, LatEur)\n", "        inEu1 = inContinent(lon, lat, LonEu1, LatEu1)\n", "    \n", "    if inEur==True or inEu1==True:\n", "        return 'Europe'\n", "    \n", "    else:\n", "        inAsi = inContinent(lon, lat, LonAsi, LatAsi)\n", "        inAs2 = inContinent(lon, lat, LonAs2, LatAs2)\n", "    \n", "    if inAsi==True or inAs2==True:\n", "        return 'Asia'\n", "    else:\n", "        inAfr = inContinent(lon, lat, LonAfr, LatAfr)\n", "        inAf1 = inContinent(lon, lat, LonAf1, LatAf1)\n", "    \n", "    if inAfr==True or inAf1==True:\n", "        return 'Africa'\n", "    else:\n", "        inAnt = inContinent(lon, lat, LonAnt, LatAnt)\n", "    \n", "    if inAnt==True:\n", "        return 'Antarctica'\n", "    else:\n", "        return 'Oceania'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pyspark.sql.functions import udf\n", "continentLabeler = udf(lambda lon, lat: getContinent(lon, lat))\n", "spark.udf.register(\"continentLabeler\", continentLabeler)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_continent = data_top100.withColumn('Continent', continentLabeler('Lat', 'Long'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[18]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_continent = data_continent.drop(\"Province/State\",\"Country/Region\",\"Lat\",\"Long\",\"Linear Coef\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[19]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Group and sum the values by continent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_continent = data_continent.groupBy(\"Continent\").sum()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[21]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_date_continent = data_continent.drop(\"Continent\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[22]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["list_Europe = df_date_continent.collect()[0]\n", "list_Africa = df_date_continent.collect()[1]\n", "list_Oceania = df_date_continent.collect()[2]\n", "list_America = df_date_continent.collect()[3]\n", "list_Asia = df_date_continent.collect()[4]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[23]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_Europe=[list_Europe[0]]\n", "data_Africa=[list_Africa[0]]\n", "data_Oceania=[list_Oceania[0]]\n", "data_America=[list_America[0]]\n", "data_Asia=[list_Asia[0]]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[24]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["reates a list of sublists. Each one of the sublist contain 7 days"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["list_eu = [list_Europe[i:i + 7] for i in range(0, len(list_Europe), 7)]\n", "list_af = [list_Africa[i:i + 7] for i in range(0, len(list_Africa), 7)]\n", "list_oc = [list_Oceania[i:i + 7] for i in range(0, len(list_Oceania), 7)]\n", "list_am = [list_America[i:i + 7] for i in range(0, len(list_America), 7)]\n", "list_as = [list_Asia[i:i + 7] for i in range(0, len(list_Asia), 7)]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[25]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mean_week_eu = [np.mean(i) for i in list_eu]\n", "std_week_eu = [np.std(i) for i in list_eu]\n", "max_week_eu = [max(i) for i in list_eu]\n", "min_week_eu = [min(i) for i in list_eu]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mean_week_af = [np.mean(i) for i in list_af]\n", "std_week_af = [np.std(i) for i in list_af]\n", "max_week_af = [max(i) for i in list_af]\n", "min_week_af = [min(i) for i in list_af]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mean_week_oc = [np.mean(i) for i in list_oc]\n", "std_week_oc= [np.std(i) for i in list_oc]\n", "max_week_oc = [max(i) for i in list_oc]\n", "min_week_oc = [min(i) for i in list_oc]\n", "    \n", "mean_week_am = [np.mean(i) for i in list_am]\n", "std_week_am = [np.std(i) for i in list_am]\n", "max_week_am = [max(i) for i in list_am]\n", "min_week_am = [min(i) for i in list_am]\n", "    \n", "mean_week_as = [np.mean(i) for i in list_as]\n", "std_week_as = [np.std(i) for i in list_as]\n", "max_week_as = [max(i) for i in list_as]\n", "min_week_as = [min(i) for i in list_as]\n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[26]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_eu_mean = pd.DataFrame(mean_week_eu ,columns = [\"Mean\"])\n", "df_eu_std = pd.DataFrame(std_week_eu ,columns = [\"Std\"])\n", "df_eu_max = pd.DataFrame(max_week_eu ,columns = [\"Max\"])\n", "df_eu_min = pd.DataFrame(min_week_eu ,columns = [\"Min\"])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["result_eu = pd.concat([df_eu_mean, df_eu_std, df_eu_max, df_eu_min], axis = 1)\n", "result_eu.index = np.arange(1, len(result_eu) + 1)\n", "result_eu.index.name = 'Europe'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[27]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_af_mean = pd.DataFrame(mean_week_af ,columns = [\"Mean\"])\n", "df_af_std = pd.DataFrame(std_week_af ,columns = [\"Std\"])\n", "df_af_max = pd.DataFrame(max_week_af ,columns = [\"Max\"])\n", "df_af_min = pd.DataFrame(min_week_af ,columns = [\"Min\"])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["result_af = pd.concat([df_af_mean, df_af_std, df_af_max, df_af_min], axis = 1)\n", "result_af.index = np.arange(1, len(result_af) + 1)\n", "result_af.index.name = 'Afrique'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[28]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_oc_mean = pd.DataFrame(mean_week_oc ,columns = [\"Mean\"])\n", "df_oc_std = pd.DataFrame(std_week_oc ,columns = [\"Std\"])\n", "df_oc_max = pd.DataFrame(max_week_oc ,columns = [\"Max\"])\n", "df_oc_min = pd.DataFrame(min_week_oc ,columns = [\"Min\"])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["result_oc = pd.concat([df_oc_mean, df_oc_std, df_oc_max, df_oc_min], axis = 1)\n", "result_oc.index = np.arange(1, len(result_oc) + 1)\n", "result_oc.index.name = 'Oceania'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[29]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_am_mean = pd.DataFrame(mean_week_am ,columns = [\"Mean\"])\n", "df_am_std = pd.DataFrame(std_week_am ,columns = [\"Std\"])\n", "df_am_max = pd.DataFrame(max_week_am ,columns = [\"Max\"])\n", "df_am_min = pd.DataFrame(min_week_am ,columns = [\"Min\"])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["result_am = pd.concat([df_am_mean, df_am_std, df_am_max, df_am_min], axis = 1)\n", "result_am.index = np.arange(1, len(result_am) + 1)\n", "result_am.index.name = 'America'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[30]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_as_mean = pd.DataFrame(mean_week_as ,columns = [\"Mean\"])\n", "df_as_std = pd.DataFrame(std_week_as ,columns = [\"Std\"])\n", "df_as_max = pd.DataFrame(max_week_as ,columns = [\"Max\"])\n", "df_as_min = pd.DataFrame(min_week_as ,columns = [\"Min\"])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["result_as = pd.concat([df_as_mean, df_as_std, df_as_max, df_as_min], axis = 1)\n", "result_as.index = np.arange(1, len(result_as) + 1)\n", "result_as.index.name = 'Asia'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["###  2nd Query's output"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[33]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import matplotlib.patches as mpatches"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_func(df_eu,df_af,df_oc,df_am,df_as,xlabel,ylabel,title):\n", "    '''\n", "    \n", "    Parameters\n", "    ----------\n", "    df_eu : List\n", "        European values.\n", "    df_af : List\n", "        African values.\n", "    df_oc : List\n", "        European values.\n", "    df_am : List\n", "        America values.\n", "    df_as : List\n", "        Asia values.\n", "    xlabel : String\n", "        x axis label.\n", "    ylabel : String\n", "        y axis label.\n", "    title : String\n", "        Plot title .\n", "    Returns\n", "    -------\n", "    Plot\n", "        Plot data depending on different continents.\n", "    '''\n", "    \n", "    fig=plt.figure()\n", "    ax=fig.add_axes([0,0,1,1])\n", "    Europe = mpatches.Patch(color='r', label='Europe')\n", "    Africa = mpatches.Patch(color='b', label='Africa')\n", "    Oceania = mpatches.Patch(color='b', label='Oceania')\n", "    America = mpatches.Patch(color='g', label='America')\n", "    Asia = mpatches.Patch(color='black', label='Asia')\n", "    plt.legend(handles=[Europe,Africa, Oceania ,America, Asia])\n", "    ax.plot(range(len(df_eu)), df_eu, color='r')\n", "    ax.plot(range(len(df_af)), df_af, color='b')\n", "    ax.plot(range(len(df_oc)), df_af, color='orange')\n", "    ax.plot(range(len(df_am)), df_am, color='g')\n", "    ax.plot(range(len(df_as)), df_as, color='violet')\n", "    ax.set_xlabel(xlabel)\n", "    ax.set_ylabel(ylabel)\n", "    ax.set_title(title)\n", "    return plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[34]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["onfirmed Cases per Day per Continent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Yeu = list_Europe\n", "Yaf = list_Africa\n", "Yoc = list_Oceania\n", "Yam = list_America\n", "Yas = list_Asia"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_lab = \"Number of days\"\n", "y_lab = \"Confirmed Cases\"\n", "title = \"Confirmed Cases per Day per Continent\"\n", "plot_func(Yeu,Yaf,Yoc,Yam,Yas,x_lab,y_lab,title)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[35]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Mean of the Daily Cases per weeks per Continent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Yeu = df_eu_mean\n", "Yaf = df_af_mean\n", "Yoc = df_oc_mean\n", "Yam = df_am_mean\n", "Yas = df_as_mean"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_lab = 'Number of weeks'\n", "y_lab = 'Mean of the Daily Cases'\n", "title = 'Mean of the Daily Cases per weeks per Continent'\n", "plot_func(Yeu,Yaf,Yoc,Yam,Yas,x_lab,y_lab,title)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[36]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["tandard Deviation of the Daily Cases per weeks per Continent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Yeu = df_eu_std\n", "Yaf = df_af_std\n", "Yoc = df_oc_std\n", "Yam = df_am_std\n", "Yas = df_as_std"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_lab = 'Number of weeks'\n", "y_lab = 'Standard Deviation of the Daily Cases'\n", "title = 'Standard Deviation of the Daily Cases per weeks per Continent'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_func(Yeu,Yaf,Yoc,Yam,Yas,x_lab,y_lab,title)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[37]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["inimum of Daily Cases per weeks per Continent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Yeu = abs(df_eu_min)\n", "Yaf = df_af_min\n", "Yoc = df_oc_min\n", "Yam = df_am_min\n", "Yas = df_as_min"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_lab = 'Number of weeks'\n", "y_lab = 'Minimum of Daily Cases'\n", "title = 'Minimum of Daily Cases per weeks per Continent'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_func(Yeu,Yaf,Yoc,Yam,Yas,x_lab,y_lab,title)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[38]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Yeu = df_eu_min\n", "Yaf = df_af_max\n", "Yoc = df_oc_max\n", "Yam = df_am_max\n", "Yas = df_as_max"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_lab = 'Number of weeks'\n", "y_lab = 'Maximum of Daily Cases'\n", "title = 'Maximum of Daily Cases per weeks per Continent'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_func(Yeu,Yaf,Yoc,Yam,Yas,x_lab,y_lab,title)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "\"\"\"\n", "Created on Tue Dec  7 17:23:31 2021"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["@author: Raska\n", "\"\"\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["% Importing libraries"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import findspark\n", "findspark.init()\n", "import pyspark\n", "findspark.find()\n", "from pyspark.sql.functions import col\n", "from pyspark.sql import SparkSession"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Comment out if not using cluster<br>\n", "conf = pyspark.SparkConf()<br>\n", "conf.setMaster(\"spark://login1-sinta-hbc:7077\").setAppName(\"jupyter\") #comment out if not using cluster"]}, {"cell_type": "markdown", "metadata": {}, "source": ["spark = pyspark.sql.SparkSession.builder \\<br>\n", "    .master(\"spark://login1-sinta-hbc:7077\") \\<br>\n", "    .appName(\"jupyter\") \\<br>\n", "    .getOrCreate()"]}, {"cell_type": "markdown", "metadata": {}, "source": [" Local configuration"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["conf = pyspark.SparkConf().setAppName('SparkApp').setMaster('local')\n", "sc = pyspark.SparkContext(conf=conf)\n", "spark = SparkSession(sc)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pyspark.sql import functions as F \n", "from pyspark.sql.window import Window\n", "from pyspark.sql.functions import row_number, lit\n", "from pyspark.sql.functions import udf\n", "from pyspark.sql import types as T\n", "import pandas as pd"]}, {"cell_type": "markdown", "metadata": {}, "source": ["% Defining all functions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def getMonthlyIncreases(data):\n", "    ### Wrapper function to run PySpark calculations of monthly increases\n", "    # Dropping unwanted columns and grouping by countries\n", "    df = data\n", "    w = Window.partitionBy(lit(1)).orderBy(lit(1))\n", "    country_region = df.select(df.columns[:4])\n", "    data_by_dates = df.select(df.columns[4:])\n", "    \n", "    #Renaming date columns after groupby\n", "    headers_before_groupby =  data.select(data.columns[4:]).columns\n", "    headers_after_groupby = data_by_dates.columns\n", "    mapping = dict(zip(headers_after_groupby,headers_before_groupby))\n", "    renamed_frame_afterGroupby = data_by_dates.select([F.col(c).alias(mapping.get(c, c)) for c in data_by_dates.columns])\n", "    \n", "    #Getting the last date in a month as the other days are redundant for finding the mean\n", "    lastdates = []\n", "    for i in range(len(headers_before_groupby)):\n", "        try:\n", "            if headers_before_groupby[i+1].split(\"/\")[0] != headers_before_groupby[i].split(\"/\")[0]:\n", "                lastdates.append(headers_before_groupby[i])\n", "        except:\n", "            lastdates.append(headers_before_groupby[-1])\n", "    #Concatenating the number of days in each month    \n", "    lstdat = []\n", "    for i in lastdates:\n", "        if i == '1/31/20':\n", "            lstdat.append(i+\"-8\"+\"-days\")\n", "        else:\n", "            lstdat.append(i+\"-\"+i.split(\"/\")[1]+\"-days\")\n", "            \n", "    Data_datesLast = renamed_frame_afterGroupby.select(lastdates)\n", "    #Renaming multiple columns\n", "    mapping = dict(zip(lastdates,lstdat))\n", "    Last_dates_data = Data_datesLast.select([F.col(c).alias(mapping.get(c, c)) for c in Data_datesLast.columns])\n", "    \n", "    #Initializing variables to assist in joining dataframes\n", "    ls = Last_dates_data.columns\n", "    fin = Last_dates_data.select(Last_dates_data.columns[0])\n", "    DF1 = fin.withColumn(\"row_id\", row_number().over(w))\n", "    \n", "    #Looping though the Dataframe and calculating the mean, and then joining\n", "    for i in range(len(ls)):\n", "        if i == 0:\n", "            pass\n", "        else:\n", "            increases_fullframe = Last_dates_data.withColumn(ls[i],(F.col(ls[i])-F.col(ls[i-1])))\n", "            increases_singleframe = increases_fullframe.select(increases_fullframe.columns[i])\n", "            DF3 = increases_singleframe.withColumn(\"row_id\", row_number().over(w))\n", "            DF1 = DF1.join(DF3, (\"row_id\"))\n", "    \n", "    #Adding an index to country/region to join\n", "    country = country_region.withColumn(\"row_id\", row_number().over(w))\n", "    \n", "    #Joining the country with the remaining dates dataframe\n", "    monthly_increases = DF1.join(country, on=\"row_id\", how='full').drop(\"row_id\")\n", "    \n", "    #Rearranging the columns and getting the final result\n", "    lst = monthly_increases.columns\n", "    monthly_increases = monthly_increases.select(lst[-4:] + lst[:-4])\n", "    monthly_increases = monthly_increases.toPandas()\n", "    return monthly_increases"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def getMonthlyAverage(data):\n", "    ### Wrapper function to run PySpark calculations of monthly average\n", "    #Dropping unwanted columns and grouping by countries\n", "    df = data\n", "    w = Window.partitionBy(lit(1)).orderBy(lit(1))\n", "    country_region = df.select(df.columns[:4])\n", "    data_by_dates = df.select(df.columns[4:])\n", "    \n", "    #Renaming date columns after groupby\n", "    headers_before_groupby =  data.select(data.columns[4:]).columns\n", "    headers_after_groupby = data_by_dates.columns\n", "    mapping = dict(zip(headers_after_groupby,headers_before_groupby))\n", "    renamed_frame_afterGroupby = data_by_dates.select([F.col(c).alias(mapping.get(c, c)) for c in data_by_dates.columns])\n", "    \n", "    #Getting the last date in a month as the other days are redundant for finding the mean\n", "    lastdates = []\n", "    for i in range(len(headers_before_groupby)):\n", "        try:\n", "            if headers_before_groupby[i+1].split(\"/\")[0] != headers_before_groupby[i].split(\"/\")[0]:\n", "                lastdates.append(headers_before_groupby[i])\n", "        except:\n", "            lastdates.append(headers_before_groupby[-1])\n", "    #Concatenating the number of days in each month    \n", "    lstdat = []\n", "    for i in lastdates:\n", "        if i == '1/31/20':\n", "            lstdat.append(i+\"-8\"+\"-days\")\n", "        else:\n", "            lstdat.append(i+\"-\"+i.split(\"/\")[1]+\"-days\") \n", "    Data_datesLast = renamed_frame_afterGroupby.select(lastdates)\n", "    #Renaming multiple columns\n", "    mapping = dict(zip(lastdates,lstdat))\n", "    Last_dates_data = Data_datesLast.select([col(c).alias(mapping.get(c, c)) for c in Data_datesLast.columns])\n", "    \n", "    #Initializing variables to assist in joining dataframes\n", "    ls = Last_dates_data.columns\n", "    fin = Last_dates_data.select(Last_dates_data.columns[0])\n", "    DF1 = fin.withColumn(\"row_id\", row_number().over(w))\n", "    \n", "    #Looping though the Dataframe and calculating the mean, and then joining\n", "    for i in range(len(ls)):\n", "        if i == 0:\n", "            pass\n", "        else:\n", "            increases_fullframe = Last_dates_data.withColumn(ls[i],(F.col(ls[i])-F.col(ls[i-1]))/int(ls[i].split(\"-\")[1]))\n", "            increases_singleframe = increases_fullframe.select(increases_fullframe.columns[i])\n", "            DF3 = increases_singleframe.withColumn(\"row_id\", row_number().over(w))\n", "            DF1 = DF1.join(DF3, (\"row_id\"))\n", "    \n", "    #Adding an index to country/region to join\n", "    country = country_region.withColumn(\"row_id\", row_number().over(w))\n", "    \n", "    #Joining the country with the remaining dates dataframe\n", "    mean_values = DF1.join(country, on=\"row_id\", how='full').drop(\"row_id\")\n", "    \n", "    #Rearranging the columns and getting the final result\n", "    lst = mean_values.columns\n", "    mean_values = mean_values.select(lst[-4:] + lst[:-4])\n", "    mean_values = mean_values.toPandas()\n", "    return mean_values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def kMeansFit(*args):\n", "    \"\"\"\n", "    Function to cluster data points\n", "    Parameters\n", "    ----------\n", "    *args : tuple\n", "        Unbounded list of data points.\n", "    Returns\n", "    -------\n", "    out : string\n", "        String of cluster IDs, in place of list to support conversion into PySpark UDF.\n", "    \"\"\"\n", "    import numpy as np\n", "    import pandas as pd\n", "    k=4\n", "    np.random.seed(42)\n", "    def __normaliseValues(df):\n", "        out = []\n", "        for label, content in df.items():\n", "            feature = df[label]\n", "            numerator = feature - np.min(feature)\n", "            denominator = np.max(feature) - np.min(feature)\n", "            output = numerator/denominator\n", "            out.append(output)\n", "        out = np.array(out).T\n", "        out = pd.DataFrame.from_records(out)\n", "        out.columns = df.columns\n", "        return out    \n", "    def __updateCentroid(y, centroids, centroids_pointwise):\n", "        centroid_update = []\n", "        for i in range(len(centroids)):\n", "            centroid_y = 0\n", "            count = 1\n", "            for n in range(len(centroids_pointwise)):\n", "                if centroids_pointwise[n] == centroids[i]:\n", "                    centroid_y += y[n]\n", "                    count +=1\n", "            centroid_final = centroid_y/count\n", "            centroid_update.append(centroid_final)\n", "        return np.array(centroid_update)\n", "            \n", "    def __getPointwiseCentroid(y, centroids):\n", "        centroids_pointwise = []\n", "        for n in range(len(y)):\n", "            distance_list = []\n", "            y_i = y[n]\n", "            for i in range(len(centroids)):\n", "                distance = np.sqrt((y_i-centroids[i])**2)\n", "                distance_list.append(distance)\n", "            distance_list = np.array(distance_list)\n", "            assigned_centroid = centroids[np.argmin(distance_list)]\n", "            centroids_pointwise.append(assigned_centroid)\n", "        return np.array(centroids_pointwise)\n", "    df = []\n", "    for value in args:\n", "        df.append(value)\n", "    df = pd.DataFrame(df)\n", "    df = __normaliseValues(df)\n", "    y = df.mean(axis=1)\n", "    centroids = y.sample(k)\n", "    centroids.reset_index(drop=True, inplace=True)\n", "    count = 1\n", "    tol = None\n", "    while True:\n", "        centroids_pointwise = __getPointwiseCentroid(y, centroids)\n", "        centroids = __updateCentroid(y, centroids, centroids_pointwise)\n", "        avg = np.mean(centroids)\n", "        if tol is not None and avg == tol:\n", "            break\n", "        tol = avg\n", "        count+=1 \n", "    out = __getPointwiseCentroid(y, centroids)\n", "    _, out = np.unique(out, return_inverse=True)\n", "    out = list(out)\n", "    out = str(out)\n", "    return out"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def convertClusteringOutput(clusters):\n", "    \"\"\"\n", "    Converts string output from kMeansFit into array.\n", "    Parameters\n", "    ----------\n", "    clusters : string\n", "        String form of cluster identification.\n", "    Returns\n", "    -------\n", "    out : Pandas dataframe\n", "        Dataframe form of cluster identification.\n", "    \"\"\"\n", "    cluster_array = np.array(clusters)\n", "    cluster_array = list(cluster_array)\n", "    out = []\n", "    for i in cluster_array:\n", "        strsize = len(i[0])\n", "        x = i[0][1:(strsize-1)]\n", "        x = list(map(int, x.split(',')))\n", "        out.append(x)\n", "    out = pd.DataFrame(out)\n", "    out = out.T\n", "    out.set_axis(['01/20', '02/20', '03/20', '04/20', '05/20', '06/20', '07/20', '08/20', '09/20','10/20', '11/20', '12/20', '01/21', '02/21', '03/21', '04/21', '05/21', '06/21','07/21', '08/21', '09/21', '10/21', '11/21'], axis=1, inplace=True)\n", "    return out\n", "#%%\n", "def normaliseValuesStd(df):\n", "    \"\"\"\n", "    Normalises values between 0 and 1\n", "    Parameters\n", "    ----------\n", "    df : pandas Dataframe\n", "        Data points to be normalised.\n", "    Returns\n", "    -------\n", "    out : pandas Dataframe\n", "        Normalised data points.\n", "    \"\"\"\n", "    out = []\n", "    for label, content in df.items():\n", "        feature = df[label]\n", "        numerator = feature - np.min(feature)\n", "        denominator = np.max(feature) - np.min(feature)\n", "        output = numerator/denominator\n", "        out.append(output)\n", "    out = np.array(out).T\n", "    out = pd.DataFrame.from_records(out)\n", "    out.columns = df.columns\n", "    return out "]}, {"cell_type": "markdown", "metadata": {}, "source": ["% Obtainining monthly increases and monthly averages and  cleaning"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["monthly_increases = getMonthlyIncreases(data)\n", "mean_values = getMonthlyAverage(data)\n", "monthly_increases = spark.createDataFrame(monthly_increases)\n", "mean_values = spark.createDataFrame(mean_values)\n", "df_vals = data.drop('Province/State', 'Country/Region', 'Lat', 'Long')\n", "df_headers = data.select('Province/State', 'Country/Region', 'Lat', 'Long')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["% Applying linear trendline coefficient calculation and sorting<br>\n", "onvert to UDF"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["getLinearTrendlineCoef = udf(lambda *args: linearTrendlineCoefficient(*args), T.FloatType())\n", "#Selecting columns for trendline\n", "w = Window.partitionBy(lit(1)).orderBy(lit(1))\n", "df_coef = monthly_increases.select(monthly_increases.columns[4:])\n", "#Fitting trendline \n", "df_coef = df_coef.withColumn('linear_coef', getLinearTrendlineCoef(*[F.col(i) for i in df_coef.columns]))\n", "#Sorting output\n", "df_coef = df_coef.withColumn(\"row_id\", row_number().over(w))\n", "df_vals = df_vals.withColumn(\"row_id\", row_number().over(w))\n", "df_headers = df_headers.withColumn(\"row_id\", row_number().over(w))\n", "mean_values = mean_values.withColumn(\"row_id\", row_number().over(w))\n", "df_headers = df_headers.join(df_coef.select('linear_coef','row_id'), on='row_id', how='full_outer')\n", "df_mean = df_headers.join(mean_values.drop('Province/State', 'Country/Region', 'Lat', 'Long'), on=\"row_id\", how='full_outer').drop(\"row_id\")\n", "df_mean = df_mean.sort(F.col(\"linear_coef\").desc())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["% Taking only top 50 values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["top50 = df_mean.limit(50)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["% Transposing output out of PySpark to ease computation of Kmeans"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["top50_p = top50.toPandas()\n", "top50_p = top50_p.drop(columns=['Province/State', 'Country/Region', 'Lat', 'Long', 'linear_coef'])\n", "top50_p = top50_p.T\n", "top50_T = spark.createDataFrame(top50_p)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["% K-means clustering"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["getKmeansCluster = udf(lambda *args: kMeansFit(*args), T.StringType())\n", "#Fitting cluster \n", "top50_T = top50_T.withColumn('cluster_id', getKmeansCluster(*[F.col(i) for i in top50_T.columns]))\n", "# Sorting cluster output\n", "clusters = top50_T.select('cluster_id').toPandas()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["% Converting output and joining with data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_clusterID = convertClusteringOutput(clusters)\n", "df_clusterID = spark.createDataFrame(df_clusterID)\n", "df_clusterID = df_clusterID.withColumn(\"row_id\", row_number().over(w))\n", "top50_headers = top50.select('Province/State', 'Country/Region', 'Lat', 'Long', 'linear_coef').withColumn(\"row_id\", row_number().over(w))\n", "df_clusterID = top50_headers.join(df_clusterID, on='row_id', how='full').drop('row_id')\n", "pdClusterID = df_clusterID.toPandas()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["% Writing  output to CSV"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["months = ['01/20', '02/20', '03/20', '04/20', '05/20', '06/20', '07/20', '08/20', '09/20','10/20', '11/20', '12/20', '01/21', '02/21', '03/21', '04/21', '05/21', '06/21','07/21', '08/21', '09/21', '10/21', '11/21']\n", "pdClusterID.to_csv('cluster_out.csv')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["% Sorting output for plotting"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["monthlyIncTop50 = df_headers.join(df_coef.drop('linear_coef'), on=\"row_id\", how='full_outer').drop(\"row_id\")\n", "monthlyIncTop50 = monthlyIncTop50.sort(F.col(\"linear_coef\").desc()).limit(50)\n", "pdMonthlyInc = monthlyIncTop50.toPandas()\n", "pdMonthlyMean= top50.toPandas()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["% Visualisation "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vis_month_selection = -1\n", "cluster_colour = pdClusterID.iloc[:, vis_month_selection]\n", "vis_data = pd.concat((pdMonthlyMean['linear_coef'], pdMonthlyMean.iloc[:, vis_month_selection]), axis=1)\n", "# print(pdClusterID.iloc[:, vis_month_selection].name)\n", "vis_data = np.array(vis_data)\n", "from scipy.spatial import ConvexHull"]}, {"cell_type": "markdown", "metadata": {}, "source": ["lot 1: Monthly scatterplot"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def drawclusters(ax,  X, labels, colours, ncluster=4):\n", "    \"\"\"\n", "    Draws clusters and fits a convex hull to ease visualisation. A convex hull\n", "    is the smallest convex boundary of the cluster cloud.\n", "    Parameters\n", "    ----------\n", "    ax : plt object\n", "        matplotlib object instance.\n", "    X : numpy array\n", "        scatter data to be plotted.\n", "    labels : numpy array\n", "        data labels for each point.\n", "    colours : list\n", "        list of selected clusters.\n", "    ncluster : int, optional\n", "        Number of convex hull instances to generate. The default is 4.\n", "    Returns\n", "    -------\n", "    None.\n", "    \"\"\"\n", "    for i in range(ncluster):\n", "        points = X[labels == i]\n", "        ax.scatter(points[:, 0], points[:, 1], s=30, c=colours[i], label=f'Cluster {i}')\n", "        ax.legend()\n", "        hull = ConvexHull(points)\n", "        vert = np.append(hull.vertices, hull.vertices[0])  # close the polygon by appending the first point at the end\n", "        ax.plot(points[vert, 0], points[vert, 1], '--', c=colours[i])\n", "        ax.fill(points[vert, 0], points[vert, 1], c=colours[i], alpha=0.2)\n", "        ax.set_xlabel('Overall daily linear coefficient')\n", "        ax.set_ylabel('Mean of daily cases in a month')\n", "        ax.set_title('Clustering of daily rates of month' )\n", "        \n", "fig, ax = plt.subplots(1, figsize=(7, 5))\n", "colours = ['red', 'green', 'blue', 'orange']\n", "drawclusters(ax, vis_data, cluster_colour, colours, ncluster=4)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["lot 2: Seaborn heatmap plotting"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import seaborn as sns\n", "heatmap_out = pdClusterID.drop(columns=['Country/Region', 'Province/State', 'Lat', 'Long', 'linear_coef'])\n", "plt.figure(figsize=(16, 10))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cmap = sns.color_palette(\"coolwarm\", 4)\n", "g = sns.heatmap(heatmap_out, cmap=cmap, linewidth=0.05, linecolor='lightgrey',  cbar_kws={\"ticks\":[0, 1, 2, 3]}, square=True)\n", "g.set_yticklabels(pdClusterID['Country/Region'])\n", "g.set_xticklabels(months, rotation = 80)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.xlabel('')\n", "plt.ylabel('')\n", "plt.savefig('globalclust.pdf')\n", "#%%"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}